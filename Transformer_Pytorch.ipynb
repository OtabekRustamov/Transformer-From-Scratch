{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Embedding Layer\n",
    "The embedding layer provides each token in a corpus with a corresponding vector representation. This is the first layer that each sequence must be passed through. Each token in each sequence has to be embedded in a vector with a length of d_model. The input into this layer is (batch_size, seq_length). The output is (batch_size, seq_length, d_model)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          vocab_size:     size of vocabulary\n",
    "          d_model:        dimension of embeddings\n",
    "        \"\"\"\n",
    "        # inherit from nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding look-up table\n",
    "        self.lut = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # dimension of embeddings\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:              input Tensor (batch_size, seq_length)\n",
    "        Returns:\n",
    "                          embedding vector\n",
    "        \"\"\"\n",
    "        # embeddings by constant sqrt(d_model)\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Encodong\n",
    "![image.png](attachment:2419f273-e325-4cb8-82a5-083a807948f0.png)\n",
    "\n",
    "These embedded sequences are then positionally encoded to provide additional context to each word. This also allows for a single word to have varying meanings depending on its placement in the sentence. The input to the layer is (batch_size, seq_length, d_model). The positional encoding matrix, with a size of (max_length, d_model), must be sliced to the same length as each sequence in the batch, giving it a size of (seq_length, d_model). This same matrix is broadcast and added to each sequence in the batch to ensure consistency. The final output is (batch_size, seq_length, d_model)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          d_model:      dimension of embeddings\n",
    "          dropout:      randomly zeroes-out some of the input\n",
    "          max_length:   max sequence length\n",
    "        \"\"\"\n",
    "        # inherit from Module\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create tensor of 0s\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        # create position column\n",
    "        k = torch.arange(0, max_length).unsqueeze(1)\n",
    "\n",
    "        # calc divisor for positional encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # calc sine on even indices\n",
    "        pe[:, 0::2] = torch.sin(k * div_term)\n",
    "\n",
    "        # calc cosine on odd indices\n",
    "        pe[:, 1::2] = torch.cos(k * div_term)\n",
    "\n",
    "        # add dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # buffer are saved in state_dict but not trained by the optimizer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:        embeddings (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "                    embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "\n",
    "        # perform dropout\n",
    "        return self.dropout(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image.png](attachment:f10015d2-2c51-45c9-a025-8246bb780898.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model:      dimension of embeddings\n",
    "            n_heads:      number of self attention heads\n",
    "            dropout:      probability of dropout occurring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0  # ensure an even num of heads\n",
    "        self.d_model = d_model  # 512 dim\n",
    "        self.n_heads = n_heads  # 8 heads\n",
    "        self.d_key = d_model // n_heads  # assume d_value equals d_key\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)  # query weights\n",
    "        self.Wk = nn.Linear(d_model, d_model)  # key weights\n",
    "        self.Wv = nn.Linear(d_model, d_model)  # value weights\n",
    "        self.Wo = nn.Linear(d_model, d_model)  # output weights\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)  # initialize dropout layer\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           query:         query vector         (batch_size, q_length, d_model)\n",
    "           key:           key vector           (batch_size, k_length, d_model)\n",
    "           value:         value vector         (batch_size, s_length, d_model)\n",
    "           mask:          mask for decoder\n",
    "\n",
    "        Returns:\n",
    "           output:        attention values     (batch_size, q_length, d_model)\n",
    "           attn_probs:    softmax scores       (batch_size, n_heads, q_length, k_length)\n",
    "        \"\"\"\n",
    "        assert isinstance(key, torch.Tensor), \"Expected key to be a tensor\"\n",
    "        assert isinstance(query, torch.Tensor), \"Expected query to be a tensor\"\n",
    "        assert isinstance(value, torch.Tensor), \"Expected value to be a tensor\"\n",
    "\n",
    "        batch_size = key.size(0)\n",
    "\n",
    "        # calculate query, key, and value tensors\n",
    "        Q = self.Wq(query)  # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "        K = self.Wk(key)  # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "        V = self.Wv(value)  # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "\n",
    "        # split each tensor into n-heads to compute attention\n",
    "\n",
    "        # query tensor\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
    "        # (32, 10, 8, 64) -> (32, 8, 10, 64) = (batch_size, n_heads, q_length, d_key)\n",
    "\n",
    "        # key tensor\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
    "        # (32, 10, 8, 64) -> (32, 8, 10, 64) = (batch_size, n_heads, k_length, d_key)\n",
    "\n",
    "        # value tensor\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
    "        # (32, 10, 8, 64) -> (32, 8, 10, 64) = (batch_size, n_heads, v_length, d_key)\n",
    "\n",
    "        # computes attention\n",
    "        # scaled dot product -> QK^{T}\n",
    "        scaled_dot_prod = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_key)\n",
    "        # (32, 8, 10, 64) x (32, 8, 64, 10) -> (32, 8, 10, 10) = (batch_size, n_heads, q_length, k_length)\n",
    "\n",
    "        # fill those positions of product as (-1e10) where mask positions are 0\n",
    "        if mask is not None:\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # apply softmax\n",
    "        attn_probs = torch.softmax(scaled_dot_prod, dim=-1)\n",
    "\n",
    "        # multiply by values to get attention\n",
    "        A = torch.matmul(self.dropout(attn_probs), V)  # (32, 8, 10, 10) x (32, 8, 10, 64) -> (32, 8, 10, 64)\n",
    "        # (batch_size, n_heads, q_length, k_length) x (batch_size, n_heads, v_length, d_key) -> (batch_size, n_heads, q_length, d_key)\n",
    "\n",
    "        # reshape attention back to (32, 10, 512)\n",
    "        A = A.permute(0, 2, 1, 3).contiguous()\n",
    "        # (32, 8, 10, 64) -> (32, 10, 8, 64)\n",
    "\n",
    "        A = A.view(batch_size, -1, self.n_heads * self.d_key)\n",
    "        # (32, 10, 8, 64) -> (32, 10, 8*64) -> (32, 10, 512) = (batch_size, q_length, d_model)\n",
    "\n",
    "        # push through the final weight layer\n",
    "        output = self.Wo(A)  # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "\n",
    "        return output, attn_probs  # return attn_probs for visualization of the scores\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Position-Wise Feed-Forward Network\n",
    "\n",
    "After being passed through layer normalization and undergoing residual addition, the output from the attention mechanism is passed to the FFN. The FFN consists of two linear layers with a ReLU activation function. The first layer has a shape of (d_model, d_ffn). This is broadcast across each sequence of the (batch_size, seq_length, d_model) tensor, and it allows the model to learn more about each sequence. The tensor has a shape of (batch_size, seq_length, d_ffn) at this point, and it is passed through ReLU. Then, it is passed through the second layer, which has a shape of (d_ffn, d_model). This contracts the tensor to its original size, (batch_size, seq_length, d_model). The outputs are passed through layer normalization and undergo residual addition."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model:      dimension of embeddings\n",
    "            d_ffn:        dimension of feed-forward network\n",
    "            dropout:      probability of dropout occurring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:            output from attention (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            expanded-and-contracted representation (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # w_1(x).relu(): (batch_size, seq_length, d_model) x (d_model,d_ffn) -> (batch_size, seq_length, d_ffn)\n",
    "        # w_2(w_1(x).relu()): (batch_size, seq_length, d_ffn) x (d_ffn, d_model) -> (batch_size, seq_length, d_model)\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Normalization\n",
    "   ![image.png](attachment:1bf02fbd-6b7e-4907-a9e6-0bdd7eb655cc.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image.png](attachment:4555c958-a1bc-470d-9d2a-05bc97526e30.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,feature, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # initialize gamma to be all ones\n",
    "        self.gamma = nn.Parameter(torch.ones(feature))\n",
    "        # initialize beta to be all zeros\n",
    "        self.beta = nn.Parameter(torch.zeros(feature))\n",
    "        # initialize epsilon\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, src):\n",
    "        # mean of the token embeddings\n",
    "        mean = src.mean(-1, keepdim=True)\n",
    "        # variance of the token embeddings  \n",
    "        var = src.var(-1,keepdim=True,unbiased=False)\n",
    "        # return the normalized value \n",
    "        return self.gamma*(src - mean)/torch.sqrt(var + self.eps)+self.beta     "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model:      dimension of embeddings\n",
    "            n_heads:      number of heads\n",
    "            d_ffn:        dimension of feed-forward network\n",
    "            dropout:      probability of dropout occurring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # multi-head attention sublayer\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        # layer norm for multi-head attention\n",
    "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        self.positionwise_ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        # layer norm for position-wise ffn\n",
    "        self.ffn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src:          positionally embedded sequences   (batch_size, seq_length, d_model)\n",
    "            src_mask:     mask for the sequences            (batch_size, 1, 1, seq_length)\n",
    "        Returns:\n",
    "            src:          sequences after self-attention    (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # pass embeddings through multi-head attention\n",
    "        _src, attn_probs = self.attention(src, src, src, src_mask)\n",
    "\n",
    "        # residual add and norm\n",
    "        src = self.attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        _src = self.positionwise_ffn(src)\n",
    "\n",
    "        # residual add and norm\n",
    "        src = self.ffn_layer_norm(src + self.dropout(_src))\n",
    "        return src, attn_probs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model:      dimension of embeddings\n",
    "            n_heads:      number of heads\n",
    "            d_ffn:        dimension of feed-forward network\n",
    "            dropout:      probability of dropout occurring\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # masked multi-head attention sublayer\n",
    "        self.masked_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        # layer norm for masked multi-head attention\n",
    "        self.masked_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # multi-head attention sublayer\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        # layer norm for multi-head attention\n",
    "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        self.positionwise_ffn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
    "        # layer norm for position-wise fnn\n",
    "        self.fnn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg: Tensor, src: Tensor, trg_mask: Tensor, src_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg:          embedded sequences                (batch_size, trg_seq_length, d_model)\n",
    "            src:          embedded sequences                (batch_size, src_seq_length, d_model)\n",
    "            trg_mask:     mask for the sequences            (batch_size, 1, trg_seq_length, trg_seq_length)\n",
    "            src_mask:     mask for the sequences            (batch_size, 1, 1, src_seq_length)\n",
    "\n",
    "        Returns:\n",
    "            trg:          sequences after self-attention    (batch_size, trg_seq_length, d_model)\n",
    "            attn_probs:   self-attention softmax scores     (batch_size, n_heads, trg_seq_length, src_seq_length)\n",
    "        \"\"\"\n",
    "        # pass trg embeddings through masked multi-head attention\n",
    "        _trg, attn_probs = self.masked_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # residual add and norm\n",
    "        trg = self.masked_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # pass trg and src embeddings multi-head attention\n",
    "        _trg, attn_probs = self.attention(trg, src, src, src_mask)\n",
    "\n",
    "        # residual add and norm\n",
    "        trg = self.attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        _trg = self.positionwise_ffn(trg)\n",
    "\n",
    "        # residual add and norm\n",
    "        trg = self.fnn_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attn_probs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_embeddings = Embeddings(src_vocab_size, d_model)\n",
    "        self.decoder_embeddings = Embeddings(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for layer in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for layer in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def make_mask(self, src, tgt):\n",
    "        # assign 1 to tokens that need attended to and 0 to padding tokens, then add 2 dimensions\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # assign True to tokens that need attended to and False to padding tokens, then add 2 dimensions\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "\n",
    "        # generate subsequent mask\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "\n",
    "        # bitwise \"and\" operator | 0 & 0 = 0, 1 & 1 = 1, 1 & 0 = 0\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # create source and target masks\n",
    "        src_mask, tgt_mask = self.make_mask(src, tgt)\n",
    "\n",
    "        # push the src through the encoder layers\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embeddings(src)))\n",
    "\n",
    "        # decoder output and attention probabilities\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embeddings(tgt)))\n",
    "\n",
    "        # pass the sequences through each encoder\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output,_ = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # pass the sequences through each decoder\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output,_ = dec_layer(dec_output, enc_output, tgt_mask, src_mask)\n",
    "\n",
    "        # set output layer\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Set random seed and device\n",
    "random_seed = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "data_path = 'spa.txt'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Preprocess data\n",
    "lines = [line.split('\\t') for line in lines]\n",
    "lines = ['\\t'.join(line[:2]) for line in lines]\n",
    "\n",
    "# Create train, val, test split\n",
    "train_lines, val_test_lines = train_test_split(lines, test_size=0.2, random_state=random_seed, shuffle=True)\n",
    "val_lines, test_lines = train_test_split(val_test_lines, test_size=0.5, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Tokenizers\n",
    "SRC_LANGUAGE = \"en\"\n",
    "TGT_LANGUAGE = \"es\"\n",
    "tokenizer = {\n",
    "    SRC_LANGUAGE: get_tokenizer(\"spacy\", \"en_core_web_sm\"),\n",
    "    TGT_LANGUAGE: get_tokenizer(\"spacy\", \"es_core_news_sm\")\n",
    "}\n",
    "\n",
    "\n",
    "# Tokenize lines\n",
    "def tokenize_lines(lines, src_tokenizer, tgt_tokenizer):\n",
    "    tokenized_lines = []\n",
    "    for line in lines:\n",
    "        src, tgt = line.split('\\t')\n",
    "        src_tokens = src_tokenizer(src)\n",
    "        tgt_tokens = tgt_tokenizer(tgt)\n",
    "        tokenized_lines.append((src_tokens, tgt_tokens))\n",
    "    return tokenized_lines\n",
    "\n",
    "\n",
    "train_data = tokenize_lines(train_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
    "val_data = tokenize_lines(val_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
    "test_data = tokenize_lines(test_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
    "\n",
    "# Build vocabularies\n",
    "special_symbols = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "src_vocab_size = 10_000\n",
    "tgt_vocab_size = 10_000\n",
    "\n",
    "\n",
    "def yield_tokens(data, lang_idx):\n",
    "    for tokens in data:\n",
    "        yield tokens[lang_idx]\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "vocab[SRC_LANGUAGE] = build_vocab_from_iterator(yield_tokens(train_data, lang_idx=0), min_freq=1,\n",
    "                                                specials=special_symbols, special_first=True, max_tokens=src_vocab_size)\n",
    "vocab[TGT_LANGUAGE] = build_vocab_from_iterator(yield_tokens(train_data, lang_idx=1), min_freq=1,\n",
    "                                                specials=special_symbols, special_first=True, max_tokens=tgt_vocab_size)\n",
    "vocab[SRC_LANGUAGE].set_default_index(1)  # UNK_IDX\n",
    "vocab[TGT_LANGUAGE].set_default_index(1)  # UNK_IDX\n",
    "\n",
    "# Define collate function\n",
    "PAD_IDX = 0\n",
    "BOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "def collate_fn(batch, vocab):\n",
    "    batch_size = len(batch)\n",
    "    srcs, tgts = zip(*batch)\n",
    "    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
    "    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        src_indices = [BOS_IDX] + [vocab[SRC_LANGUAGE][token] for token in srcs[i]] + [EOS_IDX]\n",
    "        tgt_indices = [BOS_IDX] + [vocab[TGT_LANGUAGE][token] for token in tgts[i]] + [EOS_IDX]\n",
    "        src_len = len(src_indices)\n",
    "        tgt_len = len(tgt_indices)\n",
    "        src_vectors[i, :src_len] = torch.tensor(src_indices[:max_seq_len], dtype=torch.long)\n",
    "        tgt_vectors[i, :tgt_len] = torch.tensor(tgt_indices[:max_seq_len], dtype=torch.long)\n",
    "\n",
    "    return src_vectors, tgt_vectors\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "def calculate_accuracy(output, target, pad_idx):\n",
    "    output_flat = output.argmax(dim=-1).reshape(-1)\n",
    "    target_flat = target.reshape(-1)\n",
    "    non_pad_elements = target_flat != pad_idx\n",
    "    correct = output_flat.eq(target_flat) & non_pad_elements\n",
    "    accuracy = correct.sum().float() / non_pad_elements.sum().float()\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "src_vocab_size = 10_000\n",
    "tgt_vocab_size = 10_000\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 1024\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "num_epochs = 3\n",
    "pad_idx = 0\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length,\n",
    "                          dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\\n------------------------------\")\n",
    "    transformer.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        src_data, tgt_data = data\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = transformer(src_data, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().reshape(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().reshape(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        accuracy = calculate_accuracy(output, tgt_data[:, 1:], pad_idx)\n",
    "        epoch_accuracy += accuracy\n",
    "        batch_count += 1\n",
    "        print(f\"Batch: {batch_count}, Training Loss: {loss.item()}, Training Accuracy: {accuracy}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1}, Average Training Loss: {epoch_loss / batch_count}, Average Training Accuracy: {epoch_accuracy / batch_count}\")\n",
    "\n",
    "    transformer.eval()\n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_accuracy = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src_data, tgt_data = data\n",
    "            with autocast():\n",
    "                output = transformer(src_data, tgt_data[:, :-1])\n",
    "                loss = criterion(output.contiguous().reshape(-1, tgt_vocab_size),\n",
    "                                 tgt_data[:, 1:].contiguous().reshape(-1))\n",
    "\n",
    "            epoch_val_loss += loss.item()\n",
    "            accuracy = calculate_accuracy(output, tgt_data[:, 1:], pad_idx)\n",
    "            epoch_val_accuracy += accuracy\n",
    "            batch_count += 1\n",
    "            print(f\"Batch: {batch_count}, Validation Loss: {loss.item()}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1}, Average Validation Loss: {epoch_val_loss / batch_count}, Average Validation Accuracy: {epoch_val_accuracy / batch_count}\")\n",
    "\n",
    "    torch.save(transformer.state_dict(), f'./transformer_state_dict_epoch_{epoch + 1}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}