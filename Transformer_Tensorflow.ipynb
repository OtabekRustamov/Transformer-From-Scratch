{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef13a57a-8b4c-49af-ada8-25d6cdd15d71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bd3e0-7b42-4050-a597-c25842be0654",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a62221b-0074-4dc0-8e69-ce10c62aee8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          vocab_size:     size of vocabulary\n",
    "          d_model:        dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        # embedding look-up table\n",
    "        self.lut = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # dimension of embeddings\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:              input Tensor (batch_size, seq_length)\n",
    "        Returns:\n",
    "                          embedding vector\n",
    "        \"\"\"\n",
    "        # embeddings by constant sqrt(d_model)\n",
    "        return self.lut(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73dca56-c43b-46b5-ba38-70c2a4f60d76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Positional Encodong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a312aa0-9c17-42f3-8c4f-94c2974be3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        pe_init = np.zeros((max_seq_length, d_model), dtype=np.float32)\n",
    "        position = np.arange(0, max_seq_length, dtype=np.float32)[:, np.newaxis]\n",
    "        div_term = np.power(10_000, (-np.arange(0, d_model, 2, dtype=np.float32) / d_model))\n",
    "\n",
    "        pe_init[:, 0::2] = np.sin(position * div_term)\n",
    "        pe_init[:, 1::2] = np.cos(position * div_term)\n",
    "        pe_init = pe_init[np.newaxis, :]\n",
    "\n",
    "        self.pe = tf.cast(pe_init, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        return x + self.pe[:, :seq_length, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac29511-081b-46d5-92c3-4384a40e7854",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66469379-e4bc-45df-883c-5272d370789e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = layers.Dense(d_model)\n",
    "        self.W_k = layers.Dense(d_model)\n",
    "        self.W_v = layers.Dense(d_model)\n",
    "        self.W_o = layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = tf.matmul(Q, K, transpose_b=True) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = tf.where(mask, -1e9, attn_scores)\n",
    "\n",
    "        attn_probs = tf.nn.softmax(attn_scores, axis=-1)\n",
    "        output = tf.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        return tf.transpose(tf.reshape(x, (batch_size, seq_len, self.num_heads, self.d_k)), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, d_k = x.shape\n",
    "        return tf.reshape(tf.transpose(x, perm=[0, 2, 1, 3]), (batch_size, seq_len, self.d_model))\n",
    "\n",
    "    def call(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42183872-4abb-43c5-bb02-867af0484495",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Position-Wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf40d2e9-52c7-41df-b511-e394ca18159c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PositionWiseFeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.fc2(self.fc1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3153d0fa-29bf-471c-9eac-5095f3512fbb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "114adfba-8dbf-4f41-b91b-3f61d2a7f229",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541b0f3-9c84-4fc2-b952-7193ce5d4f80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0794d11c-1a33-4b42-af65-36fcf04e9a62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.norm3 = layers.LayerNormalization()\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2700d-fcbb-45c3-8db8-c05542f4316d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cbe52f1-226b-4561-9530-24aa8d4801b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = layers.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = layers.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.fc = layers.Dense(tgt_vocab_size)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = tf.expand_dims(tf.expand_dims(src != 0, axis=1), axis=2)\n",
    "        tgt_mask = tf.expand_dims(tf.expand_dims(tgt != 0, axis=1), axis=3)\n",
    "        seq_length = tgt.shape[1]\n",
    "        nopeak_mask = tf.cast(tf.experimental.numpy.tril(tf.ones((1, seq_length, seq_length)), k=0), dtype=tf.bool)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        src, tgt = inputs\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34fbc46b-67c7-4f12-a5e8-0545a1169973",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = tf.random.uniform((64, max_seq_length), minval=1, maxval=src_vocab_size+1, dtype=tf.int32) # (batch_size, seq_length)\n",
    "tgt_data = tf.random.uniform((64, max_seq_length), minval=1, maxval=tgt_vocab_size+1, dtype=tf.int32) # (batch_size, seq_length)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c332b4d-7aba-4a67-ba25-8cb6d815a436",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da34d1f-02ef-402e-a29e-260d47ab35e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer='adam',\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_accuracy]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69663d-9587-4736-8b2e-b4d056cbcf73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "transformer.fit(x=(src_data, tgt_data[:, :-1]), y=tgt_data[:, 1:], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8db1d-b0bb-4c7c-857d-8e69196ce246",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}